{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from NGCF import NGCF\n",
    "from utility.helper import *\n",
    "# from utility.batch_test import *\n",
    "from utility.load_data import *\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import utility.metrics as metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from time import time\n",
    "import easydict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'dataset': 'evdriver',\n",
    "    'regs' : '[1e-5]',\n",
    "    'embed_size': 64,\n",
    "    'layer_size': '[64,64,64]',\n",
    "    'lr': 0.0001,\n",
    "    'save_flag': 1,\n",
    "    'pretrain': 0,\n",
    "    'batch_size': 1024,\n",
    "    'epoch': 100,\n",
    "    'verbose': 1,\n",
    "    'node_dropout': [0.1],\n",
    "    'mess_dropout': [0.1,0.1,0.1],\n",
    "    'gpu_id': 0,\n",
    "    'weights_path': './models',\n",
    "    'Ks': '[20, 40, 60, 80, 100]',\n",
    "    'test_flag':'part'})\n",
    "\n",
    "Ks = eval(args.Ks)\n",
    "cores = multiprocessing.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = 0.\n",
    "    return r, auc\n",
    "\n",
    "def get_auc(item_score, user_pos_test):\n",
    "    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "    posterior = [x[1] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = metrics.auc(ground_truth=r, prediction=posterior)\n",
    "    return auc\n",
    "\n",
    "def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "\n",
    "    K_max = max(Ks)\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    r = []\n",
    "    for i in K_max_item_score:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    auc = get_auc(item_score, user_pos_test)\n",
    "    return r, auc\n",
    "\n",
    "def get_performance(user_pos_test, r, auc, Ks):\n",
    "    precision, recall, ndcg, hit_ratio = [], [], [], []\n",
    "\n",
    "    for K in Ks:\n",
    "        precision.append(metrics.precision_at_k(r, K))\n",
    "        recall.append(metrics.recall_at_k(r, K, len(user_pos_test)))\n",
    "        ndcg.append(metrics.ndcg_at_k(r, K, user_pos_test))\n",
    "        hit_ratio.append(metrics.hit_at_k(r, K))\n",
    "\n",
    "    return {'recall': np.array(recall), 'precision': np.array(precision),\n",
    "            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}\n",
    "\n",
    "def test_one_user(x):\n",
    "    # user u's ratings for user u\n",
    "    rating = x[0]\n",
    "    #uid\n",
    "    u = x[1]\n",
    "    #user u's items in the training set\n",
    "    try:\n",
    "        training_items = data_generator.train_items[u]\n",
    "    except Exception:\n",
    "        training_items = []\n",
    "    #user u's items in the test set\n",
    "    user_pos_test = data_generator.test_set[u]\n",
    "\n",
    "    all_items = set(range(ITEM_NUM))\n",
    "\n",
    "    test_items = list(all_items - set(training_items))\n",
    "\n",
    "    if args.test_flag == 'part':\n",
    "        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n",
    "    else:\n",
    "        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n",
    "\n",
    "    return get_performance(user_pos_test, r, auc, Ks)\n",
    "\n",
    "def test(model, users_to_test, drop_flag=False, batch_test_flag=False):\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for u_batch_id in range(n_user_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start: end]\n",
    "\n",
    "        if batch_test_flag:\n",
    "            # batch-item test\n",
    "            n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
    "            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "            i_count = 0\n",
    "            for i_batch_id in range(n_item_batchs):\n",
    "                i_start = i_batch_id * i_batch_size\n",
    "                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "                item_batch = range(i_start, i_end)\n",
    "\n",
    "                if drop_flag == False:\n",
    "                    u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                                  item_batch,\n",
    "                                                                  [],\n",
    "                                                                  drop_flag=False)\n",
    "                    i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "                else:\n",
    "                    u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                                  item_batch,\n",
    "                                                                  [],\n",
    "                                                                  drop_flag=True)\n",
    "                    i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "                i_count += i_rate_batch.shape[1]\n",
    "\n",
    "            assert i_count == ITEM_NUM\n",
    "\n",
    "        else:\n",
    "            # all-item test\n",
    "            item_batch = range(ITEM_NUM)\n",
    "\n",
    "            if drop_flag == False:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                              item_batch,\n",
    "                                                              [],\n",
    "                                                              drop_flag=False)\n",
    "                rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                              item_batch,\n",
    "                                                              [],\n",
    "                                                              drop_flag=True)\n",
    "                rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "        user_batch_rating_uid = zip(rate_batch.numpy(), user_batch)\n",
    "        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision']/n_test_users\n",
    "            result['recall'] += re['recall']/n_test_users\n",
    "            result['ndcg'] += re['ndcg']/n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "            result['auc'] += re['auc']/n_test_users\n",
    "\n",
    "\n",
    "    assert count == n_test_users\n",
    "    pool.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=185336, n_items=30895\n",
      "n_interactions=852546\n",
      "n_train=667210, n_test=185336, sparsity=0.00015\n"
     ]
    }
   ],
   "source": [
    "data_generator = Data(path='../Data/evdriver', batch_size=1024)\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load adj matrix (216231, 216231) 0.35448646545410156\n"
     ]
    }
   ],
   "source": [
    "plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_test = list(data_generator.test_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185335"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_to_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "torch.manual_seed(34)\n",
    "np.random.seed(34)\n",
    "\n",
    "drop_flag = False\n",
    "batch_test_flag=False\n",
    "\n",
    "args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "model = NGCF(data_generator.n_users,\n",
    "                 data_generator.n_items,\n",
    "                 norm_adj, args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models359.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 59/91 [20:53<11:45, 22.05s/it]"
     ]
    }
   ],
   "source": [
    "result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "            'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "# pool = multiprocessing.Pool(cores)\n",
    "\n",
    "u_batch_size = BATCH_SIZE * 2\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "test_users = users_to_test\n",
    "n_test_users = len(test_users)\n",
    "n_user_batchs = n_test_users // u_batch_size + 1\n",
    "print(n_user_batchs)\n",
    "count = 0\n",
    "\n",
    "entire_rank = torch.tensor([])\n",
    "\n",
    "for u_batch_id in tqdm(range(n_user_batchs)):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "    user_batch = test_users[start: end]\n",
    "\n",
    "    if batch_test_flag:\n",
    "        # batch-item test\n",
    "        n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
    "        rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "        i_count = 0\n",
    "        for i_batch_id in range(n_item_batchs):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "            item_batch = range(i_start, i_end)\n",
    "\n",
    "            if drop_flag == False:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                                item_batch,\n",
    "                                                                [],\n",
    "                                                                drop_flag=False)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                                item_batch,\n",
    "                                                                [],\n",
    "                                                                drop_flag=True)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "            rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "            i_count += i_rate_batch.shape[1]\n",
    "\n",
    "        assert i_count == ITEM_NUM\n",
    "\n",
    "    else:\n",
    "        # all-item test\n",
    "        item_batch = range(ITEM_NUM)\n",
    "\n",
    "        if drop_flag == False:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                            item_batch,\n",
    "                                                            [],\n",
    "                                                            drop_flag=False)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "        else:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                            item_batch,\n",
    "                                                            [],\n",
    "                                                            drop_flag=True)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "    \n",
    "    entire_rank = torch.cat((entire_rank, rate_batch), dim=0)\n",
    "    user_batch_rating_uid = zip(rate_batch.numpy(), user_batch)\n",
    "#     batch_result = pool.map(test_one_user, user_batch_rating_uid)\n",
    "    batch_result = []\n",
    "    for user_rating in user_batch_rating_uid:\n",
    "        result = test_one_user(user_rating)\n",
    "        batch_result.append(result)\n",
    "\n",
    "    count += len(batch_result)\n",
    "\n",
    "    for re in batch_result:\n",
    "        result['precision'] += re['precision']/n_test_users\n",
    "        result['recall'] += re['recall']/n_test_users\n",
    "        result['ndcg'] += re['ndcg']/n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio']/n_test_users\n",
    "        result['auc'] += re['auc']/n_test_users\n",
    "\n",
    "\n",
    "assert count == n_test_users\n",
    "# pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/91 [03:32<29:19, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/91 [07:29<34:41, 29.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30/91 [11:48<36:17, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 40/91 [16:26<34:55, 41.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 50/91 [21:30<32:57, 48.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 59/91 [25:35<15:13, 28.54s/it]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(34)\n",
    "np.random.seed(34)\n",
    "\n",
    "drop_flag = False\n",
    "batch_test_flag = False\n",
    "\n",
    "args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "model = NGCF(data_generator.n_users, data_generator.n_items, norm_adj, args).to(args.device)\n",
    "\n",
    "with open('./models359.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "\n",
    "result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "          'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "u_batch_size = BATCH_SIZE * 2\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "test_users = users_to_test\n",
    "n_test_users = len(test_users)\n",
    "n_user_batches = n_test_users // u_batch_size + 1\n",
    "print(n_user_batches)\n",
    "count = 0\n",
    "\n",
    "entire_rank = torch.tensor([])\n",
    "\n",
    "# 중간 결과 저장 경로 설정\n",
    "checkpoint_path = './intermediate_results.pkl'\n",
    "\n",
    "for u_batch_id in tqdm(range(n_user_batches)):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "    user_batch = test_users[start: end]\n",
    "\n",
    "    if batch_test_flag:\n",
    "        n_item_batches = ITEM_NUM // i_batch_size + 1\n",
    "        rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "        i_count = 0\n",
    "        for i_batch_id in range(n_item_batches):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "            item_batch = range(i_start, i_end)\n",
    "\n",
    "            if drop_flag == False:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "            rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "            i_count += i_rate_batch.shape[1]\n",
    "\n",
    "        assert i_count == ITEM_NUM\n",
    "\n",
    "    else:\n",
    "        item_batch = range(ITEM_NUM)\n",
    "\n",
    "        if drop_flag == False:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "        else:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "    entire_rank = torch.cat((entire_rank, rate_batch), dim=0)\n",
    "    user_batch_rating_uid = zip(rate_batch.numpy(), user_batch)\n",
    "    \n",
    "    batch_result = []\n",
    "    for user_rating in user_batch_rating_uid:\n",
    "        result = test_one_user(user_rating)\n",
    "        batch_result.append(result)\n",
    "\n",
    "    count += len(batch_result)\n",
    "\n",
    "    for re in batch_result:\n",
    "        result['precision'] += re['precision'] / n_test_users\n",
    "        result['recall'] += re['recall'] / n_test_users\n",
    "        result['ndcg'] += re['ndcg'] / n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "        result['auc'] += re['auc'] / n_test_users\n",
    "\n",
    "    # 메모리 해제\n",
    "    del rate_batch, user_batch_rating_uid, batch_result\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 주기적으로 중간 결과 저장\n",
    "    if (u_batch_id + 1) % 10 == 0 or (u_batch_id + 1) == n_user_batches:\n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump((result, entire_rank), f)\n",
    "        print(f\"Intermediate results saved at batch {u_batch_id + 1}\")\n",
    "\n",
    "assert count == n_test_users\n",
    "\n",
    "# 최종 결과 저장\n",
    "with open(checkpoint_path, 'wb') as f:\n",
    "    pickle.dump((result, entire_rank), f)\n",
    "print(\"Final results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/91 [01:43<29:24, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/91 [03:25<27:28, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 15/91 [05:06<25:46, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/91 [06:48<24:00, 20.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 25/91 [08:29<22:18, 20.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30/91 [10:11<20:38, 20.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 35/91 [11:52<18:56, 20.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 40/91 [13:34<17:15, 20.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 45/91 [15:15<15:34, 20.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 50/91 [16:57<13:51, 20.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 55/91 [18:38<12:08, 20.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 60/91 [20:19<10:27, 20.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 65/91 [22:01<08:48, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 70/91 [23:42<07:07, 20.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 75/91 [25:24<05:25, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 80/91 [27:06<03:44, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 85/91 [28:48<02:02, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 90/91 [30:30<00:20, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [30:40<00:00, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate results saved at batch 91\n",
      "Intermediate results saved at batch final\n",
      "Final results saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(34)\n",
    "np.random.seed(34)\n",
    "\n",
    "drop_flag = False\n",
    "batch_test_flag = False\n",
    "\n",
    "args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "model = NGCF(data_generator.n_users, data_generator.n_items, norm_adj, args).to(args.device)\n",
    "\n",
    "with open('./models359.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "\n",
    "result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "          'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "u_batch_size = BATCH_SIZE * 2\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "test_users = users_to_test\n",
    "n_test_users = len(test_users)\n",
    "n_user_batches = n_test_users // u_batch_size + 1\n",
    "print(n_user_batches)\n",
    "count = 0\n",
    "\n",
    "# 중간 결과 저장 경로 설정\n",
    "checkpoint_path = './intermediate_results.pkl'\n",
    "\n",
    "def save_intermediate_results(result, batch_id, path=checkpoint_path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    print(f\"Intermediate results saved at batch {batch_id}\")\n",
    "\n",
    "for u_batch_id in tqdm(range(n_user_batches)):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "    user_batch = test_users[start: end]\n",
    "\n",
    "    if batch_test_flag:\n",
    "        n_item_batches = ITEM_NUM // i_batch_size + 1\n",
    "        rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "        i_count = 0\n",
    "        for i_batch_id in range(n_item_batches):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "            item_batch = range(i_start, i_end)\n",
    "\n",
    "            if drop_flag == False:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "            rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "            i_count += i_rate_batch.shape[1]\n",
    "\n",
    "        assert i_count == ITEM_NUM\n",
    "\n",
    "    else:\n",
    "        item_batch = range(ITEM_NUM)\n",
    "\n",
    "        if drop_flag == False:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "        else:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "    user_batch_rating_uid = zip(rate_batch.numpy(), user_batch)\n",
    "    \n",
    "    batch_result = []\n",
    "    for user_rating in user_batch_rating_uid:\n",
    "        user_result = test_one_user(user_rating)\n",
    "        batch_result.append(user_result)\n",
    "\n",
    "    count += len(batch_result)\n",
    "\n",
    "    for re in batch_result:\n",
    "        result['precision'] += re['precision'] / n_test_users\n",
    "        result['recall'] += re['recall'] / n_test_users\n",
    "        result['ndcg'] += re['ndcg'] / n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "        result['auc'] += re['auc'] / n_test_users\n",
    "\n",
    "    # 메모리 해제\n",
    "    del rate_batch, user_batch_rating_uid, batch_result\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 주기적으로 중간 결과 저장\n",
    "    if (u_batch_id + 1) % 5 == 0 or (u_batch_id + 1) == n_user_batches:\n",
    "        save_intermediate_results(result, u_batch_id + 1)\n",
    "\n",
    "assert count == n_test_users\n",
    "\n",
    "# 최종 결과 저장\n",
    "save_intermediate_results(result, 'final')\n",
    "print(\"Final results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [30:38<00:00, 20.20s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(34)\n",
    "np.random.seed(34)\n",
    "\n",
    "# Initialize flags\n",
    "drop_flag = False\n",
    "batch_test_flag = False\n",
    "\n",
    "# Define the device and model\n",
    "args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "model = NGCF(data_generator.n_users, data_generator.n_items, norm_adj, args).to(args.device)\n",
    "\n",
    "# Load model weights\n",
    "with open('./models359.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "\n",
    "# Define result dictionary\n",
    "result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "          'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "# Batch sizes\n",
    "u_batch_size = BATCH_SIZE * 2\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "# Test users\n",
    "test_users = users_to_test\n",
    "n_test_users = len(test_users)\n",
    "n_user_batches = n_test_users // u_batch_size + 1\n",
    "print(n_user_batches)\n",
    "count = 0\n",
    "\n",
    "# Initialize entire rank tensor\n",
    "entire_rank = []\n",
    "\n",
    "# Iterate over user batches\n",
    "for u_batch_id in tqdm(range(n_user_batches)):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "    user_batch = test_users[start: end]\n",
    "\n",
    "    if batch_test_flag:\n",
    "        # Batch-item test\n",
    "        n_item_batches = ITEM_NUM // i_batch_size + 1\n",
    "        rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "        i_count = 0\n",
    "        for i_batch_id in range(n_item_batches):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "            item_batch = range(i_start, i_end)\n",
    "\n",
    "            if not drop_flag:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "\n",
    "            rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "            i_count += i_rate_batch.shape[1]\n",
    "\n",
    "        assert i_count == ITEM_NUM\n",
    "\n",
    "    else:\n",
    "        # All-item test\n",
    "        item_batch = range(ITEM_NUM)\n",
    "\n",
    "        if not drop_flag:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "        else:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "\n",
    "    # Append results to entire_rank list\n",
    "    entire_rank.append(rate_batch)\n",
    "\n",
    "    user_batch_rating_uid = zip(rate_batch, user_batch)\n",
    "\n",
    "    # Simulate multiprocessing pool.map\n",
    "    batch_result = []\n",
    "    for user_rating in user_batch_rating_uid:\n",
    "        re = test_one_user(user_rating)\n",
    "        batch_result.append(re)\n",
    "\n",
    "    count += len(batch_result)\n",
    "\n",
    "    for re in batch_result:\n",
    "        result['precision'] += re['precision'] / n_test_users\n",
    "        result['recall'] += re['recall'] / n_test_users\n",
    "        result['ndcg'] += re['ndcg'] / n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "        result['auc'] += re['auc'] / n_test_users\n",
    "\n",
    "assert count == n_test_users\n",
    "\n",
    "# Save the entire_rank as a .npz file\n",
    "np.savez_compressed('entire_rank.npz', *entire_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [30:20<00:00, 20.01s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Save the entire_rank in smaller chunks\n",
    "def save_batches(entire_rank, batch_size=1024, save_dir='batches'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    num_batches = len(entire_rank) // batch_size + (1 if len(entire_rank) % batch_size != 0 else 0)\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        np.savez_compressed(f'{save_dir}/batch_{i}.npz', *entire_rank[start:end])\n",
    "\n",
    "# Load batches one by one and process them\n",
    "def load_batches(batch_dir='batches'):\n",
    "    batch_files = sorted([os.path.join(batch_dir, f) for f in os.listdir(batch_dir) if f.endswith('.npz')])\n",
    "    entire_rank = []\n",
    "    for batch_file in tqdm(batch_files):\n",
    "        loaded_batch = np.load(batch_file)\n",
    "        batch_data = [loaded_batch[key] for key in loaded_batch]\n",
    "        entire_rank.append(np.concatenate(batch_data, axis=0))\n",
    "    return torch.tensor(np.concatenate(entire_rank, axis=0))\n",
    "\n",
    "# Example usage\n",
    "torch.manual_seed(34)\n",
    "np.random.seed(34)\n",
    "\n",
    "drop_flag = False\n",
    "batch_test_flag = False\n",
    "\n",
    "args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "model = NGCF(data_generator.n_users, data_generator.n_items, norm_adj, args).to(args.device)\n",
    "\n",
    "with open('./models359.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "\n",
    "result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "          'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "u_batch_size = BATCH_SIZE * 2\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "test_users = users_to_test\n",
    "n_test_users = len(test_users)\n",
    "n_user_batches = n_test_users // u_batch_size + 1\n",
    "print(n_user_batches)\n",
    "count = 0\n",
    "\n",
    "entire_rank = []\n",
    "\n",
    "for u_batch_id in tqdm(range(n_user_batches)):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "    user_batch = test_users[start:end]\n",
    "\n",
    "    if batch_test_flag:\n",
    "        n_item_batches = ITEM_NUM // i_batch_size + 1\n",
    "        rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "        i_count = 0\n",
    "        for i_batch_id in range(n_item_batches):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "            item_batch = range(i_start, i_end)\n",
    "\n",
    "            if not drop_flag:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "\n",
    "            rate_batch[:, i_start:i_end] = i_rate_batch\n",
    "            i_count += i_rate_batch.shape[1]\n",
    "\n",
    "        assert i_count == ITEM_NUM\n",
    "\n",
    "    else:\n",
    "        item_batch = range(ITEM_NUM)\n",
    "\n",
    "        if not drop_flag:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "        else:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "\n",
    "    entire_rank.append(rate_batch)\n",
    "    user_batch_rating_uid = zip(rate_batch, user_batch)\n",
    "\n",
    "    batch_result = []\n",
    "    for user_rating in user_batch_rating_uid:\n",
    "        re = test_one_user(user_rating)\n",
    "        batch_result.append(re)\n",
    "\n",
    "    count += len(batch_result)\n",
    "\n",
    "    for re in batch_result:\n",
    "        result['precision'] += re['precision'] / n_test_users\n",
    "        result['recall'] += re['recall'] / n_test_users\n",
    "        result['ndcg'] += re['ndcg'] / n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "        result['auc'] += re['auc'] / n_test_users\n",
    "\n",
    "assert count == n_test_users\n",
    "\n",
    "# Save the entire_rank in batches\n",
    "save_batches(entire_rank)\n",
    "\n",
    "# Load the entire_rank from batches\n",
    "entire_rank = load_batches()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [30:44<00:00, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All batches saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(34)\n",
    "np.random.seed(34)\n",
    "\n",
    "# Initialize flags\n",
    "drop_flag = False\n",
    "batch_test_flag = False\n",
    "\n",
    "# Define the device and model\n",
    "args.device = torch.device('cuda:' + str(args.gpu_id))\n",
    "model = NGCF(data_generator.n_users, data_generator.n_items, norm_adj, args).to(args.device)\n",
    "\n",
    "# Load model weights\n",
    "with open('./models359.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "\n",
    "# Define result dictionary\n",
    "result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "          'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "# Batch sizes\n",
    "u_batch_size = BATCH_SIZE * 2\n",
    "i_batch_size = BATCH_SIZE\n",
    "\n",
    "# Test users\n",
    "test_users = users_to_test\n",
    "n_test_users = len(test_users)\n",
    "n_user_batches = n_test_users // u_batch_size + 1\n",
    "print(n_user_batches)\n",
    "count = 0\n",
    "\n",
    "# Initialize entire rank list\n",
    "entire_rank = []\n",
    "batch_count = 0\n",
    "save_count = 0\n",
    "\n",
    "# Directory to save batches\n",
    "output_dir = 'batches'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over user batches\n",
    "for u_batch_id in tqdm(range(n_user_batches)):\n",
    "    start = u_batch_id * u_batch_size\n",
    "    end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "    user_batch = test_users[start: end]\n",
    "\n",
    "    if batch_test_flag:\n",
    "        # Batch-item test\n",
    "        n_item_batches = ITEM_NUM // i_batch_size + 1\n",
    "        rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "        i_count = 0\n",
    "        for i_batch_id in range(n_item_batches):\n",
    "            i_start = i_batch_id * i_batch_size\n",
    "            i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "            item_batch = range(i_start, i_end)\n",
    "\n",
    "            if not drop_flag:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "\n",
    "            rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "            i_count += i_rate_batch.shape[1]\n",
    "\n",
    "        assert i_count == ITEM_NUM\n",
    "\n",
    "    else:\n",
    "        # All-item test\n",
    "        item_batch = range(ITEM_NUM)\n",
    "\n",
    "        if not drop_flag:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=False)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "        else:\n",
    "            u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch, item_batch, [], drop_flag=True)\n",
    "            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu().numpy()\n",
    "\n",
    "    # Append results to entire_rank list\n",
    "    entire_rank.append(rate_batch)\n",
    "    batch_count += 1\n",
    "\n",
    "    # Save every 5 batches\n",
    "    if batch_count == 5:\n",
    "        save_path = os.path.join(output_dir, f'entire_rank_batch_{save_count}.npy')\n",
    "        np.save(save_path, np.array(entire_rank))\n",
    "        entire_rank = []  # Reset the list\n",
    "        batch_count = 0\n",
    "        save_count += 1\n",
    "\n",
    "    user_batch_rating_uid = zip(rate_batch, user_batch)\n",
    "\n",
    "    # Simulate multiprocessing pool.map\n",
    "    batch_result = []\n",
    "    for user_rating in user_batch_rating_uid:\n",
    "        re = test_one_user(user_rating)\n",
    "        batch_result.append(re)\n",
    "\n",
    "    count += len(batch_result)\n",
    "\n",
    "    for re in batch_result:\n",
    "        result['precision'] += re['precision'] / n_test_users\n",
    "        result['recall'] += re['recall'] / n_test_users\n",
    "        result['ndcg'] += re['ndcg'] / n_test_users\n",
    "        result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "        result['auc'] += re['auc'] / n_test_users\n",
    "\n",
    "assert count == n_test_users\n",
    "\n",
    "# Save remaining batches if any\n",
    "if entire_rank:\n",
    "    save_path = os.path.join(output_dir, f'entire_rank_batch_{save_count}.npy')\n",
    "    np.save(save_path, np.array(entire_rank))\n",
    "\n",
    "print(\"All batches saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire_rank from the .npz file\n",
    "loaded_entire_rank = np.load('entire_rank.npz')\n",
    "entire_rank = [loaded_entire_rank[key] for key in loaded_entire_rank]\n",
    "\n",
    "# Convert loaded NumPy arrays back to torch tensors\n",
    "entire_rank = torch.tensor(np.concatenate(entire_rank, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_entire_rank = np.load('./batches/entire_rank_batch_0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10163/847407169.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentire_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloaded_entire_rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_entire_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10163/847407169.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentire_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloaded_entire_rank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_entire_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "entire_rank = [loaded_entire_rank[key] for key in loaded_entire_rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, pickle5\n",
    "import pandas as pd\n",
    "\n",
    "with open('../Data/evdriver/preprocessed/meta.pickle', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "with open('../Data/evdriver/preprocessed/drivers.pickle', 'rb') as f:\n",
    "    train_dv = pickle.load(f)\n",
    "\n",
    "with open('../Data/evdriver/preprocessed/testset_df.pickle', 'rb') as f:\n",
    "    test_dv = pickle.load(f)\n",
    "\n",
    "test_dv = test_dv.sort_values(by=['Driver']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(result, driver_id, test_dv, meta, criteria):\n",
    "    location = test_dv.loc[test_dv['Driver'] == driver_id][criteria].values[0]\n",
    "    teststat = test_dv.loc[test_dv['Driver'] == driver_id]['statid'].values[0]\n",
    "    speed = test_dv.loc[test_dv['Driver'] == driver_id]['speed'].values\n",
    "    if len(speed) == 1:\n",
    "        flag = True\n",
    "    else:\n",
    "        flag = False\n",
    "\n",
    "    driver_pref = result[driver_id]\n",
    "    #[1011, 14000, 13000] -> statid의 list\n",
    "    driver_rank = driver_pref.argsort()[::-1].astype(str)\n",
    "    #하나의 Cluster로 위치 필터링\n",
    "    driver_meta = meta[meta[criteria] == location]\n",
    "    recommended_df = pd.DataFrame([])\n",
    "\n",
    "    for i in range(len(driver_rank)):\n",
    "        if len(recommended_df) > 19:\n",
    "            false_count = len(recommended_df) - recommended_df.duplicated('statid').sum()\n",
    "            if false_count == 20:\n",
    "                break\n",
    "        #추천된 statid가 Cluster에 속하면,\n",
    "        if driver_rank[i] in driver_meta['statid'].values:\n",
    "            stat_driver = driver_meta.loc[driver_meta['statid'] == driver_rank[i]]\n",
    "            #추천된 statid의 speed가 testset의 speed와 같으면,\n",
    "            if flag:\n",
    "                if stat_driver['speed'].values[0] == speed:\n",
    "                    recommended_df = pd.concat([recommended_df, stat_driver])\n",
    "            else:\n",
    "                recommended_df = pd.concat([recommended_df, stat_driver])\n",
    "                \n",
    "    if flag:\n",
    "        recommended_df = recommended_df.loc[recommended_df['speed'] == speed[0]].reset_index(drop=True)\n",
    "    else:\n",
    "        recommended_df = recommended_df.drop_duplicates('statid').reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        rank = recommended_df.loc[recommended_df['statid'] == teststat].index[0] \n",
    "        rank = rank + 1\n",
    "    except:\n",
    "        rank = 0\n",
    "    return recommended_df, rank, len(recommended_df)\n",
    "\n",
    "def get_coordinates(driver_id, meta, test_dv, candidate):\n",
    "    gt_zscode = test_dv.loc[test_dv['Driver'] == driver_id]['zscode'].values[0]\n",
    "    gt_cluster = test_dv.loc[test_dv['Driver'] == driver_id]['Cluster'].values[0]\n",
    "    gt_speeds = test_dv.loc[test_dv['Driver'] == driver_id]['speed']\n",
    "    if len(gt_speeds.values) == 1:\n",
    "        gt_speed = gt_speeds.values[0]\n",
    "    else:\n",
    "        gt_speed = gt_speeds.value_counts().keys()[0]\n",
    "    \n",
    "    meta_zscode = meta[meta['zscode'] == gt_zscode]\n",
    "    meta_zscode = meta_zscode.loc[meta_zscode['speed'] == gt_speed]\n",
    "    meta_cluster = meta.loc[meta['Cluster'] == gt_cluster]\n",
    "    meta_cluster = meta_cluster.loc[meta_cluster['speed'] == gt_speed]\n",
    "\n",
    "    zscode_cor = meta_zscode[['lat','lng']].values\n",
    "    cluster_cor = meta_cluster[['lat','lng']].values\n",
    "    candidate_cor = candidate[['lat','lng']].values\n",
    "    ground_truth_cor = test_dv.loc[test_dv['Driver'] == driver_id][['lat','lng']].values\n",
    "    \n",
    "\n",
    "    return zscode_cor, cluster_cor, candidate_cor, ground_truth_cor\n",
    "\n",
    "def show_plot(zscode_cor, cluster_cor, candidate_cor, gt_cor, driver_id):\n",
    "    plt.scatter(zscode_cor[:, 1], zscode_cor[:, 0], marker='o', color='blue', label='zscode', s=5)\n",
    "    plt.scatter(cluster_cor[:, 1], cluster_cor[:, 0], marker='o', color='red', label='cluster', s=5)\n",
    "    plt.scatter(candidate_cor[:, 1], candidate_cor[:, 0], marker='o', color='green', label='candidate', s=5)\n",
    "    plt.scatter(gt_cor[:, 1], gt_cor[:, 0], marker='x', color='black', label='Ground Truth', s=30)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(f'Scatter Plot of Coordinates for driver: {driver_id}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visulization(result, driver_id, test_dv, meta, criteria):\n",
    "    recommended_df, rank, length = evaluation(result, driver_id, test_dv, meta, criteria)\n",
    "    zscode_cor, cluster_cor, candidate_cor, gt_cor = get_coordinates(driver_id, meta, test_dv, recommended_df)\n",
    "    show_plot(zscode_cor, cluster_cor, candidate_cor, gt_cor, driver_id)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "201275    1\n",
       "201276    0\n",
       "201277    0\n",
       "201278    0\n",
       "201279    1\n",
       "Name: speed, Length: 201280, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dv['speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'speed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'speed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10163/4005790238.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisulization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentire_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cluster'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_10163/1948155983.py\u001b[0m in \u001b[0;36mvisulization\u001b[0;34m(result, driver_id, test_dv, meta, criteria)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisulization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mrecommended_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mzscode_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_cor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coordinates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommended_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mshow_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzscode_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_cor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10163/1948155983.py\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(result, driver_id, test_dv, meta, criteria)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrecommended_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommended_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecommended_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'speed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mrecommended_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommended_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'speed'"
     ]
    }
   ],
   "source": [
    "visulization(entire_rank, 0, test_dv, meta, 'Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulization(entire_rank, 19755, test_dv, meta, 'Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--weights_path [WEIGHTS_PATH]]\n",
      "                             [--data_path [DATA_PATH]]\n",
      "                             [--proj_path [PROJ_PATH]] [--dataset [DATASET]]\n",
      "                             [--pretrain PRETRAIN] [--verbose VERBOSE]\n",
      "                             [--epoch EPOCH] [--embed_size EMBED_SIZE]\n",
      "                             [--layer_size [LAYER_SIZE]]\n",
      "                             [--batch_size BATCH_SIZE] [--regs [REGS]]\n",
      "                             [--lr LR] [--model_type [MODEL_TYPE]]\n",
      "                             [--adj_type [ADJ_TYPE]] [--gpu_id GPU_ID]\n",
      "                             [--node_dropout_flag NODE_DROPOUT_FLAG]\n",
      "                             [--node_dropout [NODE_DROPOUT]]\n",
      "                             [--mess_dropout [MESS_DROPOUT]] [--Ks [KS]]\n",
      "                             [--save_flag SAVE_FLAG] [--test_flag [TEST_FLAG]]\n",
      "                             [--report REPORT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-10ecc2d8-026b-4769-8035-f495499407fa.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "from utility.parser import parse_args\n",
    "from utility.batch_test import *\n",
    "from utility.load_data import *\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "args = parse_args()\n",
    "seed = args.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def load_adjacency_list_data(adj_mat):\n",
    "    tmp = adj_mat.tocoo()\n",
    "    all_h_list = list(tmp.row)\n",
    "    all_t_list = list(tmp.col)\n",
    "    all_v_list = list(tmp.data)\n",
    "\n",
    "    return all_h_list, all_t_list, all_v_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    *********************************************************\n",
    "    Prepare the log file\n",
    "    \"\"\"\n",
    "    curr_time = datetime.datetime.now()\n",
    "    if not os.path.exists('log'):\n",
    "        os.mkdir('log')\n",
    "    logger = logging.getLogger('train_logger')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logfile = logging.FileHandler('log/{}.log'.format(args.dataset), 'a', encoding='utf-8')\n",
    "    logfile.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    logfile.setFormatter(formatter)\n",
    "    logger.addHandler(logfile)\n",
    "\n",
    "    \"\"\"\n",
    "    *********************************************************\n",
    "    Prepare the dataset\n",
    "    \"\"\"\n",
    "    data_generator = Data(args)\n",
    "    logger.info(data_generator.get_statistics())\n",
    "\n",
    "    print(\"************************* Run with following settings 🏃 ***************************\")\n",
    "    print(args)\n",
    "    logger.info(args)\n",
    "    print(\"************************************************************************************\")\n",
    "\n",
    "    config = dict()\n",
    "    config['n_users'] = data_generator.n_users\n",
    "    config['n_items'] = data_generator.n_items\n",
    "\n",
    "    \"\"\"\n",
    "    *********************************************************\n",
    "    Generate the adj matrix\n",
    "    \"\"\"\n",
    "    plain_adj = data_generator.get_adj_mat()\n",
    "    all_h_list, all_t_list, all_v_list = load_adjacency_list_data(plain_adj)\n",
    "    config['plain_adj'] = plain_adj\n",
    "    config['all_h_list'] = all_h_list\n",
    "    config['all_t_list'] = all_t_list\n",
    "\n",
    "    \"\"\"\n",
    "    *********************************************************\n",
    "    Prepare the model and start training\n",
    "    \"\"\"\n",
    "    model = NGCF(data_generator.n_users,\n",
    "                 data_generator.n_items,\n",
    "                 norm_adj, args).to(args.device)\n",
    "    with open('./models359.pkl', 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "\n",
    "    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n",
    "              'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n",
    "\n",
    "    u_batch_size = BATCH_SIZE * 2\n",
    "    i_batch_size = BATCH_SIZE\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_user_batchs = n_test_users // u_batch_size + 1\n",
    "    print(n_user_batchs)\n",
    "    count = 0\n",
    "\n",
    "    entire_rank = torch.tensor([])\n",
    "\n",
    "    for u_batch_id in tqdm(range(n_user_batchs)):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "        user_batch = test_users[start:end]\n",
    "\n",
    "        if batch_test_flag:\n",
    "            # batch-item test\n",
    "            n_item_batchs = ITEM_NUM // i_batch_size + 1\n",
    "            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n",
    "\n",
    "            i_count = 0\n",
    "            for i_batch_id in range(n_item_batchs):\n",
    "                i_start = i_batch_id * i_batch_size\n",
    "                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n",
    "\n",
    "                item_batch = range(i_start, i_end)\n",
    "\n",
    "                if drop_flag == False:\n",
    "                    u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                                  item_batch,\n",
    "                                                                  [],\n",
    "                                                                  drop_flag=False)\n",
    "                    i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "                else:\n",
    "                    u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                                  item_batch,\n",
    "                                                                  [],\n",
    "                                                                  drop_flag=True)\n",
    "                    i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "                rate_batch[:, i_start: i_end] = i_rate_batch\n",
    "                i_count += i_rate_batch.shape[1]\n",
    "\n",
    "            assert i_count == ITEM_NUM\n",
    "\n",
    "        else:\n",
    "            # all-item test\n",
    "            item_batch = range(ITEM_NUM)\n",
    "\n",
    "            if drop_flag == False:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                              item_batch,\n",
    "                                                              [],\n",
    "                                                              drop_flag=False)\n",
    "                rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "            else:\n",
    "                u_g_embeddings, pos_i_g_embeddings, _ = model(user_batch,\n",
    "                                                              item_batch,\n",
    "                                                              [],\n",
    "                                                              drop_flag=True)\n",
    "                rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n",
    "\n",
    "        entire_rank = torch.cat((entire_rank, rate_batch), dim=0)\n",
    "        user_batch_rating_uid = zip(rate_batch.numpy(), user_batch)\n",
    "        \n",
    "        # 멀티프로세싱 대신 단일 프로세스로 평가\n",
    "        batch_result = [test_one_user(x) for x in user_batch_rating_uid]\n",
    "        count += len(batch_result)\n",
    "\n",
    "        for re in batch_result:\n",
    "            result['precision'] += re['precision'] / n_test_users\n",
    "            result['recall'] += re['recall'] / n_test_users\n",
    "            result['ndcg'] += re['ndcg'] / n_test_users\n",
    "            result['hit_ratio'] += re['hit_ratio'] / n_test_users\n",
    "            result['auc'] += re['auc'] / n_test_users\n",
    "\n",
    "    assert count == n_test_users\n",
    "\n",
    "    # 최종 결과 출력\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
